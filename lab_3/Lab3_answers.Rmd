---
title: 'Lab 3: Logistic regression and predictive models'
author: "Debby Lipschutz (modified from Stuart McGurnaghan)"
date: "05/02/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# 1. Logistic regression

Generalised Linear Models (GLM) assume that the observed outcome has been drawn from a distribution from the exponential family and that there is a linear relationships between the predictors and (a function of) the mean of that distribution. It is particularly useful with data for which we are unlikely to obtain normally distributed errors such as binary and count data. Medical data is often binary, classifying people as either having a condition (cases) or not (controls). Such binary case-control data can then be used to investigate potential risk/protective factors associated with cases/controls and make predictions. Using a GLM it is assumed that cases and controls were drawn from a binomial distribution and that the logit of the probabilitiies from the binomial distribution has a linear relationship with the predicors, i.e. $ln(p/1-p) = X\beta$ with y~B(n,p).   

Let’s consider a case-control study which includes 1327 women aged 50–81 with hip fractures as well as 3262 randomly selected women in the same age range. Among the cases, 40 women take hormone replacement therapy (HRT); among the controls, 239 women do. You are interested in finding out whether taking HRT affects the probability of having a hip fracture. Let's say the only data you have are these summary statistics. You can resolve this by creating a synthetic dataset which has the same characteristics and use that perform your analyses.
```{r}
y <- c(rep(1, 1327),rep(0, 3262)) # cases, controls
hrt <- c(rep(1, 40),rep(0, 1287), # HRT, no HRT in cases
                    rep(1, 239),rep(0, 3023)) # HRT, no HRT in controls
```


To fit a logistic regression model, we can use the generalised linear model function glm(). For data drawn from a binomial distribution with a logit function linking to the linear predictor, use the option family=binomial(link = "logit"). The logit link function is the standard option for the binomial family and therefore family="binomial" can also be entered instead but we will use the full version for clarity. Note that if nothing is specified for the family option a guassian family with the identity link function is used as standard, which is equivalent to fitting a linear regression model.
```{r}
regr.hrt <-glm(y ~ hrt, family=binomial(link="logit")) 
summary(regr.hrt)
```


Because of the presence of the link function, we have two types of fitted values. The first, correspond to the log-odds scale and are stored in the linear.predictors vector of the regression object.
```{r}
X <- model.matrix(regr.hrt)
y.hat <- as.numeric(X%*% coef(regr.hrt)) # turn a 1-column matrix into a vector
all(y.hat==regr.hrt$linear.predictors)
```


The second fitted values object correspond to the probability scale (the response scale), that is by transforming the linear predictors through the logistic function: these are stored in the fitted.values vector of the regression object.
```{r}
logistic <- function(z)exp(z)/(exp(z)+1)
prob.case <-logistic(regr.hrt$linear.predictors) 
all(prob.case==regr.hrt$fitted.values)
```

It is crucial not to mix the two scales, otherwise the interpretation will be affected. 


In logistic regression the Wald test statistic is distributed according to a normal distribution because the standard deviation of the outcome doesn’t need to be estimated (this is what the message “Dispersion parameter for binomial family taken to be 1” refers to). Therefore, the test statistic is now labelled z value(instead of t value). The p-value is obtained by comparing the test statistic to the quantiles of a standard normal distribution.
Recalling that $ln(p/1-p) = X\beta$ the estimated coefficients indicate how much the log-odds increase with each unit increase of the predictor. The sign of the regression coefficient tells us that the use of HRT has a protective effect on hip fractures (it reduces their odds). However, we need to transform it to an oddsratio in order to better appreciate the magnitude of the effect.
```{r}
# exponentiate to get the odds ratio
exp(coef(regr.hrt)[2])
```


If HRT had no effect, the odds ratio would be 1. An odds ratio of 0.393 indicates that HRT reduces the odds of having a hip fracture by 60.7%.To build a 95% confidence interval on the odds ratio, we can rely on a normal approximation remembering that 95% of the probability mass of a standard normal distribution is within -1.96 and 1.96, $\exp({\hatβ_i}±1.96×SE(\hatβ_i))$.
```{r}
beta <- regr.hrt$coefficients[2]
se.beta <- coef(summary(regr.hrt))[2, 2]
round(exp(beta+1.96*se.beta* c(-1, 1)), 3)
```


Equivalently, it is possible to use the confint() function. Results are not exactly the same, as the computation used above is based on asymptotic normality, while confint() uses a t-distribution, which is usually better, especially for smaller sample sizes. Again, the intervals are in the log-odds scale, but we are interested in confidence intervals for the odds ratios, so we exponentiate them.
```{r}
or.ci <- round(exp(confint(regr.hrt)), 3)
or.ci
```

The confidence interval does not include 1 (the odds ratio corresponding to no effect), so we can reject the null hypothesis of no effect for HRT. 

The estimated intercept is -0.854. Transforming the log-odds through the logistic function, we can compute the baseline probability, that is the probability of an event when all other covariates (in our case, just taking HRT) are zero.
```{r}
baseline.odds <- exp(coef(regr.hrt)[1])
baseline.prob <- baseline.odds/(1+baseline.odds)
# logistic function
round(baseline.prob, 3)
```


This tells us that a woman in the study who does not take HRT has 29.9% probability of experiencing a hip-fracture. How does this probability change if a woman takes HRT?
```{r}
hrt.odds <- exp(coef(regr.hrt)[1] + coef(regr.hrt)[2])
hrt.prob <- hrt.odds/(1+hrt.odds)# logistic function
round(hrt.prob, 3)
```

Note that these probabilities do not represent the absolute risk for a generic woman aged 50–81, as the proportion of cases in a case-control study generally does not reflect the proportion in the general population. 


### Exercise 1.1
Using the chdagesex.csv dataset from Learn, fit a logistic regression model to test the association between age and coronary heart disease (model M1). Fit an additional model (M2) which also includes sex then compute the odds ratio for the predictors and the 95% confidence interval for both models.
```{r}
chdage <- read.csv("data/chdagesex.csv")

fit.m1 <- glm(CHD ~ AGE, data=chdage, family="binomial")
or.age <- exp(coef(fit.m1)[2])
ci.age <- exp(confint(fit.m1))[2, ]

round(c(or.age, ci.age), 2)
```

```{r}
fit.m2 <- glm(CHD ~ AGE + SEX, data=chdage, family="binomial")

or.age <- exp(coef(fit.m2)[2])
ci.age <- exp(confint(fit.m2))[2, ]
round(c(or.age, ci.age), 2)

or.sex <- exp(coef(fit.m2)[3])
ci.sex <- exp(confint(fit.m2))[3, ]
round(c(or.sex, ci.sex), 2)
```


# 2. Model evaluation

The log-likelihood of a fitted model can be extracted with function logLik(): this also outputs the number of degrees of freedom, that is the number of parameters estimated by the model.
```{r}
logLik(regr.hrt)
```


The deviance of a model is minus twice the log-likelihood. This is what R labels “residual deviance” when using summary() over a logistic regression object.
```{r}
-2* as.numeric(logLik(regr.hrt)) # same as regr.hrt$deviance
```


The null deviance, instead, is minus twice the log-likelihood of a model which only uses the intercept term and no other predictor. A null model reports the same predicted probabilites for all observations, corresponding to the proportion of cases in the dataset.
```{r}
null.model <-glm(y~1, family="binomial") # only the intercept in the model
head(null.model$fitted.values) # same probabilities for all observations
sum(y)/ length(y) # proportion of cases
```

```{r}
-2* as.numeric(logLik(null.model)) # same as regr$null.deviance
```


Models with better fit have lower deviance. In our case, adding one parameter to the null model decreases the deviance from 5519.71 to 5484.8. The difference between two deviances has a $χ2$ distribution with degrees of freedom equal to the difference in parameters of the two models. Therefore we can use it as a test statistic and compute a p-value under the null hypothesis that there is no support for an additional term. If the p-value is smaller than the significance level then we reject the null hypothesis and claim that the larger model provides a significantly better fit.
```{r}
pchisq(regr.hrt$null.deviance-regr.hrt$deviance, df=1, lower.tail=FALSE)
```


### Exercise 2.1
Perform a likelihood ratio test comparing model M1 to model M2 from exercise 1.1 and confirm that the addition of sex to the model is significant at  0.05 by computing a p-value.
```{r}
pval <- pchisq(fit.m1$deviance - fit.m2$deviance, df=1, lower.tail=FALSE)
signif(pval, 2)
```


The AIC is another quantity derived from the log-likelihood which is used in model comparison, as it also takes into account model complexity, the deviance of the model is penalised by twice the number of parameters estimated in the model (stored in the rank field of the regression object).
```{r}
2*regr.hrt$rank - 2* as.numeric(logLik(regr.hrt))

regr.hrt$aic
```

```{r}
AIC(null.model)
```


As for the deviance, the lower the AIC, the better the model. However, there is no formal test that can be applied when comparing the AICs of two models. Nonetheless it can be a very useful initial tool to consider inclusion of variables in a model when there is a large number of variables (we will look at this and other methodologies for large datasets in the next lab). 


The BIC (see R documentation) applies a stronger penalty for the use of additional predictors: even so, in this case the proposed model is still better than the null model.
```{r}
log(length(y))*regr.hrt$rank - 2* as.numeric(logLik(regr.hrt))

BIC(regr.hrt)
BIC(null.model)
```


# 3. Making predictions

Let’s consider a dataset which relates to subarachnoid hemorrhage (SAH), which is part of the pROC package. SAH is a rare, potentially fatal, type of stroke caused by bleeding on the surface of the brain. The dataset contains a clinical assesment score "wfns" and a couple of biomarkers "s100b" and "ndka" taken at the time of hospitalization. The patients were re-assessed 6 months later and the outcome is recorded in the clinical score "gos6", this has been forther reduced to the binary "outcome" variable. 
```{r}
library(pROC)

data(aSAH) # copy the dataset from the package into the workspace
summary(aSAH)
```

Let’s start by fitting a simple model containing only age and gender on the entire dataset.
```{r}
asah.all <-glm(outcome ~ age+gender, data=aSAH, family="binomial")
summary(asah.all)
```

The predict() function allows us to use a fitted regression model to make a prediction. The function takes two main arguments: a fitted prediction object (the output of lm() or glm()) and a dataframe (specified by option newdata) containing the covariate values for which we want to make a prediction of the outcome. By default, for a logistic regression model predict() will return the linear predictors, that is the log-odds. If we are interested in predicted probabilities, we need to specify type="response".
```{r}
y.linpred <-predict(asah.all, newdata=aSAH) # predict the data used for fitting
y.pred <-predict(asah.all, newdata=aSAH, type="response")

head(data.frame(y.linpred, asah.all$linear.predictors, y.pred, asah.all$fitted.values))
```

Predicting the same data that was used for fitting is not very interesting this produces what is already available in the linear.predictors and fitted.values vectors of the regression object. Moreover if we tried to use this to assess the predictive performance of the model, we would be overestimating it as it may be influenced by artefacts unique to his dataset. However this function is useful for predicting the outcome for new data. This needs to be a dataframe containing columns that correspond to the predictors used in the model. For example, to predict the response for a 30 years old male patient, we could create a dataframe to contain these data and ask the model to make a prediction for it.
```{r}
data.30M <-data.frame(age=30, gender="Male")
predict(asah.all, newdata=data.30M, type="response")
```

In general, predictions will be made on dataframes coming from different cohorts, or for subsets of the original dataset that were withdrawn, as in a cross-validation setting. 


### Exercise 3.1
Create dataframe agesex containing two columns:
* AGE with values in the sequence from 1 to 100
* SEX created as follows
```{r}
set.seed(1)
SEX <- factor(rbinom(100, 1, 0.5), labels=c("F", "M"))
```
Predict the probabilities of CHD for the agesex data according to model M2 and plot them using a different colour according to sex.
```{r}
agesex <- data.frame(AGE=1:100, SEX)
pred <- predict(fit.m2, type="response", newdata=agesex)
plot(agesex$AGE, pred, xlab = "Age", ylab = "Predicted probability of CHD", col = agesex$SEX)
```


## 3.1 ROC curves

A common method to evaluate the ability of a model to predict binary outcomes is the use of Receiver operating characteristic (ROC) curves. Let's imagine that the output of some linear predictor ranges between zero and ten. If we say that all predicted values greater than zero are cases then our test will be correctly classifying all the observed cases and have a specificity of 1. However, all our controls will also have been wrongly classified as cases and our specificity is therefore zero. Similarly, if we say that all predicted greater than ten are cases we would have a sensitivity of 0 and a specificty of 1. Roc curves plot the sensitivity of a test against its specificity across all threshold values.        


### Exercise 3.2
Using the hemophilia.csv dataset from Learn:
* Using as.integer(), convert the “group” variable to a 0-1 integer variable to be used as outcome variable of a classification model. Use a logistic regression to model the probability of being a carrier of hemophilia A using the two “AHF” variables as predictors, call it regr.hem and retrieve the predicted probabilities.
* Using a classification threshold θ= 0.5, count the number of misclassified observations, the answer should be 9. Derive the sensitivity and specificity at this threshold.
* Write a function called sens.spec(y.obs, y.pred, threshold) that computes sensitivity and specificity from a 0-1 vector of observed outcomes, a vector of probabilities and a threshold value, and returns the two quantities as a vector.
* Use the new function sens.spec() to compute sensitivity and specificity of regr.hem for at least 10 equally-spaced values of θ spread in the interval (0,1) and use these to plot the ROC curve for regr.hem.
```{r}
hemo <- read.csv("data/hemophilia.csv")

summary(hemo)
```
```{r}
hemo$group <- 2 - as.integer(hemo$group) # assign a 1 to carriers

regr.hem <- glm(group ~ AHFantigen + AHFactivity, data=hemo, family="binomial")
probs <- regr.hem$fitted.values

pred.case <- as.integer(probs > 0.5)
sum(pred.case != hemo$group)
```
```{r}
sens <- sum(hemo$group == 1 & pred.case == 1) / sum(hemo$group == 1)
round(sens, 2)

spec <- sum(hemo$group == 0 & pred.case == 0) / sum(hemo$group == 0)
round(spec, 2)
```
```{r}
sens.spec <- function(threshold, y.obs, y.pred) {
  sens <- sum(y.obs == 1 & (y.pred > threshold) == 1) / sum(y.obs == 1)
  spec <- sum(y.obs == 0 & (y.pred > threshold) == 0) / sum(y.obs == 0)
  return(c(spec, sens))
}

thresh <- seq(0, 1, by=0.1)
ss <- sapply(thresh, sens.spec, y.obs = hemo$group, y.pred = probs)

plot(ss[1,], ss[2, ], xlab="Specificity", ylab="Sensitivity", type="b") # plots points and joining them with lines
```

A useful feature of the ROC curve is the fact that the Area Under the Curve (AUC) gives the overall probability of correctly ranking a randomly chosen case above a randomly chosen control. If our model was completely random we would have an equal chance of ranking either the randomluy chosen case or the randomly chosen control above the other and the AUC would therefore be 0.5. To compute the AUC we can use the package pROC. This provides function roc(), which by default reports the area under curve, but offers an option to plot the entire ROC curve (see ?roc for full documentation).
```{r}
if(!"pROC"%in% rownames(installed.packages())) {install.packages("pROC")} # only install if it isn't already installed
# If you are asked to choose a CRAN mirror, pick any from the list, and when asked if you would like to use a personal library, answer “yes”.
library(pROC)
  
roc(hemo$group, regr.hem$fitted.values , plot=TRUE, xlim = c(0,1))
```


Let's look again at the HRT model.
```{r}
roc(y, regr.hrt$fitted.values, plot=TRUE, xlim = c(0,1))
```

An AUC of 0.52 is not good, this model is not very different from a random model at discriminating cases from controls. This model could help us understand if HRT has an effect on the odds of hip fractures but not to explain hip fractures based on HRT (or indeed to predict them!). 


### Exercise 3.3
Plot the ROC curves for models M1 and M2 in the same graph (hint: use option add=TRUE for the second curve) and report their AUCs.
```{r}
roc(chdage$CHD, fit.m1$fitted.values, plot=TRUE, xlim = c(0,1))
roc(chdage$CHD, fit.m2$fitted.values, plot=TRUE, xlim = c(0,1), add=TRUE, col="red")
```


## 3.2 Data partitioning and cross-validation

Some convenient functions related to predictive models are implemented in the caret package.
```{r}
if(!"caret"%in% rownames(installed.packages())) {install.packages("caret")} # Use the console to install if you get error messages
library(caret) 
```


This allows us to call function createDataPartition() which creates a single training/test split. This function (like similar ones in the same package) requires us to pass the vector of outcomes, so that the partition will contain a balanced proportion of cases and controls in the training and test sets. Before creating the partition, we must set the seed of the random number generation, so that results will be reproducible.
```{r}
set.seed(1)
train.idx <-createDataPartition(aSAH$outcome, p=0.7)$Resample1 # 70-30 split
```


This creates a list of indices corresponding to the observations in the training set. We can use this to fit the model only on this subset of data. The subset option of lm() and glm() allows control of the observations that are used to fit the model coefficients without making us create new dataframes to store the subsets.
```{r}
asah.train <-glm(outcome~age+gender, data=aSAH, subset=train.idx, family="binomial")

summary(asah.train)
```


Having fitted the same model on just a part of the data, we can see that the coefficients are different from those we obtained when we used all the data. Also p-values are less extreme, but this is mainly due to the reduced size of the dataset and the consequent increase in the standard errors. Note that we cannot use the deviance or the AIC to compare models fitted on different datasets (or, as in this case, on datasets of different sizes). These quantities depend on the number of observations used in fitting, so a model fitted to a larger dataset will in general have higher deviance and AIC, but this is not necessarily an indication of bad quality of fit. What really matters is how well we predict the outcome for the withdrawn observations.
```{r}
pred.prob <-predict(asah.train, newdata=aSAH[-train.idx, ], type="response")
```


Let’s plot the ROC curve for the prediction of the test data and compute the corresponding test AUC.
```{r}
roc(outcome~pred.prob, data=aSAH[-train.idx, ], plot=TRUE, xlim=c(0,1))
```

The AUC for this model (0.8194) is quite good. Unfortunately, given that we created only one partition, it is hard to tell if a similar performance would be obtained on a different random training/test split. K-folds cross-validation essentially repeats this process for k different subsets and allows us to make a prediction for all observations of the dataset. The folds can be generated with the function createFolds(). By default the function returns a list of indices corresponding to the test set of each fold.
```{r}
set.seed(1)
num.folds <- 10
folds <-createFolds(aSAH$outcome, k=num.folds) # get indices of the test sets
```

Note that the folds object is a list, this datatype allows to store different classes of objects (say vectors and dataframes) within the same variable. To access an element of a list, use the [[ ]] operator.
```{r}
folds[[1]]
```

Now it’s a matter of fitting a model in each of the training sets and predict the outcome for observations in the corresponding test sets. We need to store each fitted model so that they can be used afterwards for prediction or inspection: by assigning the output from glm() to res.folds[[fold]] we are effectively adding an element to a list of results.
```{r}
regr.cv <- NULL
for(f in 1:num.folds) {
  train.idx <- setdiff(1:nrow(aSAH), folds[[f]])
  regr.cv[[f]] <- glm(outcome ~ age + gender, data=aSAH, subset = train.idx, family="binomial")
  }
```


### Exercise 3.4
Write function glm.cv(formula, data, folds) that given: 
* a model formula (an expression of the type outcome ~ predictors),
* a dataframe containing outcome and predictors,
* and a set of cross-validation folds produced by createFolds(), 
fits a logistic regression model in each of the folds and returns a list of fitted models. 
After setting the random seed to 1, generate a set of 10 cross-validation folds and use glm.cv() to cross-validate model M1 and model M2.
```{r}
glm.cv <- function(formula, data, folds) {
  regr.cv <- NULL
  for (f in 1:length(folds)) {
    regr.cv[[f]] <- glm(formula, data=data[-folds[[f]], ], family="binomial") 
    # This is equivalent to doing 10, 90-10 splits we are fitting the model on all but 1/10th of the data ten times.
  }
  return(regr.cv)
}

set.seed(1675) # Canged the seed as by chance it created a fold with no cases
folds <- createFolds(chdage$CHD, k=10)  

cv.m1 <- glm.cv(CHD ~ AGE, chdage, folds)
cv.m2 <- glm.cv(CHD ~ AGE + SEX, chdage, folds)
```


We will need another loop to produce the predicted responses for each fold. For convenience we create a dataframe to store the observed outcome and what we predict from the model.
```{r, message=F}
pred.cv <- NULL
auc.cv <- numeric(num.folds)
for(f in 1:num.folds) {
  test.idx <- folds[[f]] 
  pred.cv[[f]] <- data.frame(obs = aSAH$outcome[test.idx],
                             pred = predict(regr.cv[[f]], newdata = aSAH, type = "response")[test.idx])
  auc.cv[f] <- roc(obs ~ pred, data = pred.cv[[f]])$auc
  }
```

After cross-validation, we can report the expected performance of the model on withdrawn data. This is slightly smaller than what we obtained when using a single partition, but we can attach more confidence to this estimate.
```{r}
round(mean(auc.cv), 3)
```


### Exercise 3.5
Write function predict.cv(regr.cv, data, outcome, folds) where: 
* regr.cv is a list of fitted models produced by glm.cv(), 
* data is a dataframe of covariates,
* outcome is the vector of observed outcomes,
* and folds is the set of cross-validation folds.
The function should use the model fitted on the training set of each fold to predict the outcome of the corresponding test set. The function should return a list of dataframes, each containing observed and predicted outcome for the test observations.
Usepredict.cv()to make predictions for both model M1 and model M2. Using these predictions, compute AUCs for all folds and report the mean cross-validated AUCs.
```{r, message=F}
predict.cv <- function(regr.cv, data, outcome, folds) {
  pred.cv <- NULL
  for (f in 1:length(folds)) {
    test.idx <- folds[[f]]
    pred.cv[[f]] <- data.frame(obs = outcome[test.idx],
                              pred = predict(regr.cv[[f]], 
                              newdata = data[test.idx,],
                              type = "response"))
  }
  return(pred.cv)
}

pred.cv.m1 <- predict.cv(cv.m1, chdage, chdage$CHD, folds)
pred.cv.m2 <- predict.cv(cv.m2, chdage, chdage$CHD, folds)

auc.cv.m1 <- auc.cv.m2 <- numeric(length(folds))

for (f in 1:length(folds)) {
  auc.cv.m1[f] <- roc(obs ~ pred, data=pred.cv.m1[[f]])$auc
  auc.cv.m2[f] <- roc(obs ~ pred, data=pred.cv.m2[[f]])$auc
  }

round(mean(auc.cv.m1), 3)
round(mean(auc.cv.m2), 3)
```



### Exercise 3.6
Consider the following summary statistics from a study of smoking in students:
                            | student smokes  | student does not smoke
____________________________|_________________|_________________________
at least one parent smokes  | 816             | 3203
neither parent smokes       | 188             | 1168

* Compute the odds ratio of smoking in students according to the exposure to smoking in parents directly from the values in the table.
```{r}
round((816/188) / (3203/1168), 2)
```

* Create a synthetic dataset with the same characteristics as those in the table and fit a logistic regression model to it. Check that the odds ratio for exposure to smoking in parents matches what you computed before and report the 95% confidence interval, and the Wald test p-value.
```{r}
par.smoke <- c(rep(1, 816 + 3203), rep(0, 188 + 1168))
stu.smoke <- c(rep(1, 816), rep(0, 3203), rep(1, 188), rep(0, 1168))

regr.smoke <- glm(stu.smoke ~ par.smoke, family="binomial")

round(exp(coef(regr.smoke)[2]), 2)
round(exp(confint(regr.smoke)[2, ]), 2)
signif(coef(summary(regr.smoke))[2, 4], 3)
```

* By using the deviance, test the goodness-of-fit of the model by deriving a p-value.
```{r}
signif(pchisq(regr.smoke$null.deviance - regr.smoke$deviance, df=1, lower.tail=FALSE), 2)
```

* From the regression coefficients compute the probability of smoking for a student whose parents do not smoke and for a student whose parents smoke.
```{r}
predict(regr.smoke, newdata=data.frame(par.smoke=0), type="response")
predict(regr.smoke, newdata=data.frame(par.smoke=1), type="response")
```

* Compute the sensitivity and specificity of the model for threshold values of 0.12, 0.14, 0.2 and 0.22.
```{r}
for (theta in c(0.12, 0.14, 0.2, 0.22)){
  sens <- sum(fitted(regr.smoke) > theta & stu.smoke) / sum(stu.smoke)
  
  spec <- sum(fitted(regr.smoke) < theta & (1 - stu.smoke)) / sum(1 - stu.smoke)
  print(paste0('Theta: ', theta, ', Sensitivity: ', round(sens, 3), ', Specificity: ', round(spec, 3)))
}
```
